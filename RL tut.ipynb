{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aac00a6",
   "metadata": {},
   "source": [
    "# Installing and importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75db95c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\91638\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.13.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.21.5)\n",
      "Requirement already satisfied: gym==0.21 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.0.1)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.6.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.64.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.13.0)\n",
      "Requirement already satisfied: rich in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.4.1)\n",
      "Requirement already satisfied: ale-py==0.7.4 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.7.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.8.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\91638\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (9.0.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\91638\\anaconda3\\lib\\site-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\91638\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.27.1)\n",
      "Requirement already satisfied: click in c:\\users\\91638\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\91638\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.7.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.19.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.54.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (61.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.23.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.26.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\91638\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\91638\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (4.1.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\91638\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\91638\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91638\\anaconda3\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11->stable-baselines3[extra]) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2021.3)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\91638\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11->stable-baselines3[extra]) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea8f7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.5.27 in c:\\users\\91638\\appdata\\roaming\\python\\python39\\site-packages (1.5.27)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyglet==1.5.27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb72b3f",
   "metadata": {},
   "source": [
    "The above line is what actually helps in rendering the environment without any error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffafc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from pyglet.gl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594a30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca68380",
   "metadata": {},
   "source": [
    "the main environment functions are:\n",
    "1. env.reset() - resets the environment and obtain the initial observations\n",
    "2. env.render() - visualize the environment\n",
    "3. env.step() - apply an action to the environment\n",
    "4. env.close() - close down the render frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c0e23",
   "metadata": {},
   "source": [
    "The below block of code is just to simply run the environment 10 times. It does not train the Rl model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc016dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1Score:26.0\n",
      "Episode:2Score:34.0\n",
      "Episode:3Score:23.0\n",
      "Episode:4Score:44.0\n",
      "Episode:5Score:22.0\n",
      "Episode:6Score:28.0\n",
      "Episode:7Score:11.0\n",
      "Episode:8Score:20.0\n",
      "Episode:9Score:17.0\n",
      "Episode:10Score:20.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10;\n",
    "for episode in range (1,episodes+1):\n",
    "    state = env.reset() #resetting to the initial set of observations\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()#used to visualize the environment\n",
    "        action = env.action_space.sample()#selects a random action from the action space of the environment\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score +=reward\n",
    "    print('Episode:{}Score:{}'.format(episode,score))\n",
    "# env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5ca4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()#closes the gym environment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33682361",
   "metadata": {},
   "source": [
    "For the documentation of the cartpole environment \n",
    "https://www.gymlibrary.dev/environments/classic_control/cart_pole/#arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0e8a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd13ab1",
   "metadata": {},
   "source": [
    "The above output means that there are only two possible actions . Either 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "742f59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training','Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb94408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb0049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)#creates the gym environment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy',env,verbose =1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c9d04",
   "metadata": {},
   "source": [
    "env = DummyVecEnv([lambda: env]) - it wraps the environment in a 'DummyVecEnv'. The 'DummyVecEnv' is a vectorized environment wrapper that allows us to treat a single environment as multiple parallel environments, enabling more efficient training by taking advantage of parallel processing ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61d2e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 504  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 316         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008486919 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00277     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.72        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 50.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009725066 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0916      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 33          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007128752 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 51.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006968987 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949742 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.5        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 55.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011006216 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 46.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060866484 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.588       |\n",
      "|    explained_variance   | 0.545        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.9         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00995     |\n",
      "|    value_loss           | 50.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009940883 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.48        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 34.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035544436 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.556       |\n",
      "|    explained_variance   | 0.552        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.7         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00585     |\n",
      "|    value_loss           | 73.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1ac63806430>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e69b38",
   "metadata": {},
   "source": [
    "learn() method starts the training process for the specified number of timesteps. During this training, the model interacts with the environment,collects experiences, and updates its policy to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d8dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training','Saved Models','PPO Model Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8565ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db2d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91638\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model,env,n_eval_episodes=10,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e36c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a8af7",
   "metadata": {},
   "source": [
    "# Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11b5613f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[426.]\n",
      "Episode:2 Score:[500.]\n",
      "Episode:3 Score:[500.]\n",
      "Episode:4 Score:[500.]\n",
      "Episode:5 Score:[417.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1,episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action,_ = model.predict(obs) #using the model to predict the steps\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6df1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6d18c",
   "metadata": {},
   "source": [
    "# Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daca7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path,'PPO_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a002104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO_1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "232b8313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c360d",
   "metadata": {},
   "source": [
    "Tensorboard is a web-based tool provided  by TensorFlow for visualizing training and evaluation metrics such as training curves,model graphs etc. \n",
    "\n",
    "You can access it using http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb83ff",
   "metadata": {},
   "source": [
    "# Adding a callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c534b",
   "metadata": {},
   "source": [
    "Importing the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1e4d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0c64396",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training','Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2131fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=500, verbose=1)\n",
    "eval_callback = EvalCallback(env,callback_on_new_best = stop_callback,\n",
    "                            eval_freq=10000,\n",
    "                            best_model_save_path=save_path,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef025c",
   "metadata": {},
   "source": [
    "StopTrainingOnRewardThreshold - this callback is used to stop the training process when the specified reward threshold is reached.\n",
    "\n",
    "Verbose - it is a parameter which determines whether to print the information about the training progress.verbose=1 will display the information\n",
    "\n",
    "EvalCallback - this callback is used to evaluate the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51f8b344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model =PPO('MlpPolicy',env,verbose=1,tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f7049",
   "metadata": {},
   "source": [
    "PPO - Proximal Policy Optimization model \n",
    "\n",
    "MlpPolicy - it is a policy architecture specified to the PPO model. \n",
    "Here it uses the Multi-layer perceptron policy which is a feedforward \n",
    "neural network.\n",
    "\n",
    "tensorboard - it is a visualization tool commonly used for monitoring \n",
    "and analyzing the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31854c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_4\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 491  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008331481 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.00595     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.04        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 320         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009499189 |\n",
      "|    clip_fraction        | 0.0716      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 38.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 308         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007384121 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 51          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=335.60 +/- 116.14\n",
      "Episode length: 335.60 +/- 116.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | 336          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076204482 |\n",
      "|    clip_fraction        | 0.0814       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.611       |\n",
      "|    explained_variance   | 0.387        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    value_loss           | 61.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 285   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012148925 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 63          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004438039 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00884    |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007776455 |\n",
      "|    clip_fraction        | 0.0723      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.65        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00829    |\n",
      "|    value_loss           | 39          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004199842 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.51        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 24.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=489.00 +/- 22.00\n",
      "Episode length: 489.00 +/- 22.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 489         |\n",
      "|    mean_reward          | 489         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009160817 |\n",
      "|    clip_fraction        | 0.0863      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.25        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 269   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1ac784123a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02848450",
   "metadata": {},
   "source": [
    "During the training, the model will interact with the environment, collect experiences and update its policy to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d650a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
